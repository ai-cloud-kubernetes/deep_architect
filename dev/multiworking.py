import deep_architect.utils as ut
import deep_architect.search_logging as sl

# NOTE: this code is very experimental and it is here only with the purpose of
# that it might be useful and save you the trouble of writing it yourself
# at the time that you are considering your particular use-case. APIs are
# likely to change in future versions.


# can likely be later improved, e.g., the condition for checking that the
# worker as completed the work.
class LocalWorker:
    """Simple local worker that runs a Python process locally with or without a GPU.

    Relaying a job is achieved through a DeepArchitect log folder.
    The master, where the searcher lies, writes a config file and refers
    the worker to it; the worker reads it from disk and writes the results file,
    once  the job is done; having the results written is interpreted as having the
    resources used by the worker free once again.

    .. note::
        This class assumes that the results for the job are written once to
        disk and that determines the job having been completed and the resources
        being again available.

        Currently, this class does not work properly if the worker terminates
        without writing a results file, as it will simply assume that it is
        still working. This can be addressed by making sure that the worker
        code address potential exceptions and never terminates with an error.
    """

    def __init__(self,
                 main_filepath,
                 config_filepath,
                 logs_folderpath,
                 search_name,
                 gpu_id=None):
        self.main_filepath = main_filepath
        self.config_filepath = config_filepath
        self.logs_folderpath = logs_folderpath
        self.search_name = search_name
        self.gpu_id = gpu_id

    def submit_job(self, evaluation_id):
        # NOTE: assumes a specific form for the command that does the evaluation.
        cmd = "export CUDA_VISIBLE_DEVICES=%d && python %s --config_filepath %s --evaluation_id %d &" % (
            self.gpu_id if self.gpu_id is not None else '', self.main_filepath,
            self.config_filepath, evaluation_id)
        ut.run_bash_command(cmd)

    def is_job_done(self, evaluation_id):
        p = sl.get_evaluation_folderpath(self.logs_folderpath, self.search_name,
                                         evaluation_id)
        return ut.file_exists(ut.join_paths([p, 'results.json']))


class LocalWorkerPool:
    """Aggregates many local workers into a pool and manages it.

    A worker only becomes available after the results generated by its job have
    been consumed.
    """

    def __init__(self, worker_lst, query_every_k_seconds):
        self.worker_lst = worker_lst
        self.available_worker_ids = set(range(len(worker_lst)))
        self.running_evaluation_ids = [None] * len(worker_lst)
        self.consumable_evaluation_ids = []
        self.query_every_k_seconds = query_every_k_seconds

        self.timer = ut.TimerManager()
        self.timer.create_timer("pool")
        self.timer.tick_timer("pool")

    def get_num_available_workers(self):
        if (self.timer.get_time_since_last_tick('pool', units="seconds") >=
                self.query_every_k_seconds):
            avail_ids = []
            for worker_id, worker in enumerate(self.worker_lst):
                if worker_id not in self.available_worker_ids:
                    # check completion of the job that the worker was running
                    eval_id = self.running_evaluation_ids[worker_id]
                    if worker.is_job_done(eval_id):
                        self.consumable_evaluation_ids.append(eval_id)
                        self.available_worker_ids.add(worker_id)

        return len(self.available_worker_ids)

    def get_num_consumable_evaluation_ids(self):
        return len(self.consumable_evaluation_ids)

    def consume_evaluation_ids(self):
        ids = self.consumable_evaluation_ids
        self.consumable_evaluation_ids = []
        return ids

    def submit_job(self, evaluation_id):
        assert len(self.available_worker_ids) > 0

        worker_id = self.available_worker_ids.pop()
        worker = self.worker_lst[worker_id]
        worker.submit_job(evaluation_id)
        self.running_evaluation_ids[worker_id] = evaluation_id


# TODO: I need to use a worker to make sure that it works the way I believe
# that it can work, e.g., by using slurm directly. another possibility is
# to login to the server to do what I want.

# script='#!/bin/bash'"
# SBATCH --nodes=1
# SBATCH --partition=GPU-shared
# SBATCH --gres=gpu:k80:$4
# SBATCH --cpus-per-task=$3
# SBATCH --mem=$5MB
# SBATCH --time=$6
# SBATCH --job-name=\"$2\"
# $1" && echo \"$script\" > _run.sh && chmod +x _run.sh && sbatch _run.sh && rm _run.sh;
# }

# # # TODO: this is supposed to be tools that should help running architecture
# # # search experiments at scale.

# import multiprocessing

# # def run_parallel_experiment(experiment_fn, iter_args):
# #     ps = []
# #     for args in iter_args:
# #         p = multiprocessing.Process(target=experiment_fn, args=args)
# #         p.start()
# #         ps.append(p)

# #     for p in ps:
# #         p.join()