<#> some parts may be immutable, but now I'm sort of counting on good practices 
    from the user for them not to change the internals.

<#> For MCTS with bisection or otherwise, all hyperparameters have to be discrete.

<#> Generation of the resulting graph code in strictly PyTorch or TF is going 
    to be useful to share the results. This is more problematic with respect 
    to the training code, but it is not a big problem.

<#> Think about the ability of sharing parameters across different modules. 
    This seems hard to do. It would be easier to do it directly somehow in the 
    code.

<#> Delegate the common function calls to a function that knows how to do the 
    multiplexing.

<#> Introduce a cleaner way of dealing with hyperparameters during the search
    process.

<#> Think about how to generate an encoding of the network or the search 
    process that an RL agent can use to make decisions, even if it is a simple 
    one in terms of the values of the hyperparameters somehow.

<#> Propagation of information when setting hyperparameters can be done in 
    _update for Modules and is_set for hyperparameters. They could perhaps both
    be handled in a cleaner way.

<#> Do some examples in Tensorflow, PyTorch, and DyNet. Compare with the state of 
    the art. Make two or three examples across different frameworks.

<#> Add a reasonable way of adding extra hyperparameters that are used by the 
    algorithm somewhere. Perhaps just pass to the specification algorithm.

<# TODO> Check that the copy mechanism for sampling multiple network works 
    correctly replacing the existing scope.

<# THINK> Possibility of handling different frameworks similarly, depending 
    of whether they static or dynamic. There is really not much investment from 
    the user in terms of expanding an existing class such as SISOModule.

<# THINK> How to extract the parameters of the resulting framework? How to 
    serialize the results? 

<# NOTE> More dynamic graph. A lot more graph manipulation.

<# THINK> Forward propagation and graph construction is done through inputs and 
    outputs, is there a possibility of doing it more cleanly? Not very important
    for now.

<#> Dependency between hyperparameters is not handled in the best way. Think
    about how to do it.

<#> Think about clean functionality for managing the scope.

<# THINK> Clean way of encapsulating the results of some operation.

<# TODO> Improve graph visualization. By showing hyperparameters and names of 
    connections and other things.

<# TODO> Refactor the code to make sure that it 

<#> Easy interfacing with the framework, when looking for models.

<#> More efficient way of doing forward propagation in dynamic networks.

<#> Possibility of using the same forward between backends.

<#> Easily generating a module without having to write 10-20 lines of boilerplate
    for most functionalities.

<#> Think about the implementation of interesting sharing schemes through 
    ModuleBuilder type classes. 

<#> Think about implementing a SISO module builder.

<#> It would be nice to compose things in a line. Right now it is not possible 
    due to the way the connection of the inputs is done.

<#> Introduce a function that allows to append a large fraction of some 
    graph to this model.

<#> Definition of new auxiliary functions for definition of new modules.

<#> Possibility of adding a train and test mode for all modules. This is not 
    a difficult addition.

<#> Some of these may require a training and testing mode. How to do this based 
    on a function is interesting. requires passing this information too.

design principles:
- be lazy about the instantiation of modules.
- remove no longer used modules from scope.
- keep a list of unconnected terminals: 
- can be done through the scope perhaps.
- the problem of implementing this functionality through the scope is that 

